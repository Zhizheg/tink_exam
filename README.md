# Предисловие
Данная языковая модель, если её можно так назвать, представляет из себя n-грамную модель. Была идея добавить обучаемую модель линейной регрессии, которая бы имела бы такую структуру:

С помощью предложенных библиотек и функций считались бы id токены входного слова, определялись бы фичи, тоже числовые, а в форме предсказания модели мы бы получали id токен таргета. Конечно, абсолютно такой же точен получить нереально, поэтому из списка слов выбрался бы максимально близкое по токену слово. Но у этого решения есть серьёзные проблемы:

1) Непонятен алгоритм токенизации, поэтому доверять доступным вариантам было бы опрометчиво. А если токенизация происходит случайно, то весь смысл обучения теряется

2) Возникают трудности с отбором фичей. Ведь, если подумать, количество слогов, длина слова и его место в предложении не играют роли в определении следующего слова. Единственное, что мне пришло на ум— обрабатывать окончания слов, но, учитывая всё могущество русского языка, написать адекватную функцию по выделению окончаний кажется почти невозможным и слишком затратным. Тем более, модель с одним весом слишком бесполезна и практически не уменьшит энтропию при предсказании.

Исходя из вышеупомянутых выводов, было решено делать простую n-грамную модель

Да, и о nlp моделях, предложенных в виде решения я знаю так мало, что нерационально было даже думать об их использовании)

#Реализации:

Управление файлами осуществляется через командную строку и выполнено с помощью библиотеки argparse.

Сохранение и загрузка модели сделаны с помощью dill. 

Сама модель представляет из себя класс, включающий предобработку входных данных, обучение и генерацию текста. Хранение и структуризация данных осушествлены в словаре списков.

В ходе предобработки модель удаляет все символы и знаки, кроме знаков ‘.,?!’ конца и разделения предложения. Эти знаки принимаются за отдельные слова, ключи и значения словаря. Также при генерации модель может поставить такой знак случайно, выбрав из остальных ключей, либо пробив вероятность, которая увеличивается при росте конкретного предложения. То есть, чем дольше идёт предложение, тем вероятнее следующим словом станет конец этого предложения. 

#Управление



