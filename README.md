# Предисловие
Данная языковая модель, если её можно так назвать, представляет из себя n-граммную модель. Была идея добавить обучаемую модель линейной регрессии, которая бы имела бы такую структуру:

С помощью предложенных библиотек и функций считались бы id токены входного слова, определялись бы фичи, тоже числовые, а в форме предсказания модели мы бы получали id токен таргета. Конечно, абсолютно такой же точен получить нереально, поэтому из списка слов выбрался бы максимально близкое по токену слово. Но у этого решения есть серьёзные проблемы:

1) Непонятен алгоритм токенизации, поэтому доверять доступным вариантам было бы опрометчиво. А если токенизация происходит случайно, то весь смысл обучения теряется

2) Возникают трудности с отбором фичей. Ведь, если подумать, количество слогов, длина слова и его место в предложении не играют роли в определении следующего слова. Единственное, что мне пришло на ум — обрабатывать окончания слов, но, учитывая всё могущество русского языка, написать адекватную функцию по выделению окончаний кажется почти невозможным и слишком затратным. Тем более, модель с одним весом слишком бесполезна и практически не уменьшит энтропию при предсказании.

Исходя из вышеупомянутых выводов, было решено делать простую n-граммную модель

Да, и о nlp моделях, предложенных в виде решения я знаю так мало, что нерационально было даже думать об их использовании)

# Реализации:

В общем идея наследует предложенное решение через n-граммную модель

Управление файлами осуществляется через командную строку и выполнено с помощью библиотеки argparse.

Сохранение и загрузка модели сделаны с помощью dill. 

Сама модель представляет из себя класс, включающий предобработку входных данных, обучение и генерацию текста. Хранение и структуризация данных осушествлены в словаре списков.

В ходе предобработки модель удаляет все символы и знаки, кроме знаков ‘.,?!’ конца и разделения предложения. Эти знаки принимаются за отдельные слова, ключи и значения словаря. Также при генерации модель может поставить такой знак случайно, выбрав из остальных ключей, либо пробив вероятность, которая увеличивается при росте конкретного предложения. То есть, чем дольше идёт предложение, тем вероятнее следующим словом станет конец этого предложения. 

# Управление
Для создания и обучения модули используйте модуль 'train.py'
Параметры: путь до файла с текстом или деректории файлов с текстом; путь, указывающий, куда загрузить обученную модель
Пример:
python C:/Users/X-Wing/Desktop/Felis/train.py C:/Users/X-Wing/Desktop/dta C:/Users/X-Wing/Desktop/modl/yuki.data

Для загрузки модели и генерации текста используйте модуль 'generate.py'
Параметры: путь до файла с существующей моделью; префикс - необязательный параметр, указывающий начало текста; длина текста
Пример:
python C:/Users/X-Wing/Desktop/Felis/generate.py C:/Users/X-Wing/Desktop/modl/yuki.data --prefix="ты не понимаешь" 20

Файл с моделью называется 'yuki.data'. Модель обучена на 'Пикник на обочине' Стругацких и 'Зов Ктулху' Говарда Лавкрафта.

В папке data находятся все файлы с текстами, использованными для обучения
